{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaConfig, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "def load_license_data(json_folder):\n",
    "    license_data = []\n",
    "    for filename in os.listdir(json_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            license_name = filename[:-5]\n",
    "            filepath = os.path.join(json_folder, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                license_data.append({\n",
    "                    \"license_name\": license_name,\n",
    "                    \"family\": data[\"family\"],\n",
    "                    \"labels\": data[\"labels\"],\n",
    "                    \"text\": data[\"text\"],\n",
    "                })\n",
    "    return license_data\n",
    "\n",
    "json_folder = \"../../data/processed/preprocessed_licenses_json\"\n",
    "license_data = load_license_data(json_folder)\n",
    "\n",
    "# --- Create DataFrame ---\n",
    "df = pd.DataFrame(license_data)\n",
    "\n",
    "# --- Handle Missing Labels ---\n",
    "df.dropna(subset=[\"labels\"], inplace=True)\n",
    "df = df[df[\"labels\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# --- Multi-Label Encoding ---\n",
    "mlb = MultiLabelBinarizer()\n",
    "multi_hot_labels = mlb.fit_transform(df[\"labels\"])\n",
    "df[\"multi_hot_labels\"] = list(multi_hot_labels)\n",
    "num_labels = len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 29\n",
      "Classes: ['Anti-DRM (obligation/WARNING)' 'Anti-Tivoization (obligation/WARNING)'\n",
      " 'Copyleft (network protective) (obligation/ALARM)'\n",
      " 'Copyleft (strong) (obligation/WARNING)'\n",
      " 'Copyleft (weak) (obligation/WARNING)'\n",
      " 'Declare modification in source code (obligation/WARNING)'\n",
      " 'Deprecated License (other/INFORMATION)'\n",
      " 'Display acknowledgement message (obligation/WARNING)'\n",
      " 'Display additional information (obligation/WARNING)'\n",
      " 'Display copyright notice (obligation/INFORMATION)'\n",
      " 'Display license in binary (obligation/INFORMATION)'\n",
      " 'Display license in the source (obligation/INFORMATION)'\n",
      " 'Doing Business with US (other/ALARM)'\n",
      " 'Endorsement prohibited (prohibition/INFORMATION)'\n",
      " 'Jurisdiction specific (other/WARNING)'\n",
      " 'Keep copy of source code available (obligation/WARNING)'\n",
      " 'License upgrade allowed (right/INFORMATION)'\n",
      " 'Limitation (limitation/WARNING)'\n",
      " 'No further restrictions permitted (prohibition/INFORMATION)'\n",
      " 'Patent grant (other/INFORMATION)' 'Permissive (right/INFORMATION)'\n",
      " 'Provide source code location (obligation/WARNING)'\n",
      " 'Public Domain (other/INFORMATION)'\n",
      " 'Severe patent retaliation (other/ALARM)'\n",
      " 'Standard patent retaliation (other/WARNING)'\n",
      " 'Unclear or Ambiguous (other/ALARM)'\n",
      " 'Usage notice in advertisement (obligation/INFORMATION)'\n",
      " 'Use in distributed software (right/INFORMATION)'\n",
      " 'Written offer to request source code (obligation/WARNING)']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique labels: {num_labels}\")\n",
    "print(f\"Classes: {mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(multi_hot_labels, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution before oversampling: Counter({'Display copyright notice (obligation/INFORMATION)': 220, 'Display license in the source (obligation/INFORMATION)': 220, 'Use in distributed software (right/INFORMATION)': 220, 'Display license in binary (obligation/INFORMATION)': 213, 'Permissive (right/INFORMATION)': 142, 'Endorsement prohibited (prohibition/INFORMATION)': 106, 'Declare modification in source code (obligation/WARNING)': 94, 'Keep copy of source code available (obligation/WARNING)': 76, 'Provide source code location (obligation/WARNING)': 72, 'Patent grant (other/INFORMATION)': 65, 'License upgrade allowed (right/INFORMATION)': 54, 'No further restrictions permitted (prohibition/INFORMATION)': 51, 'Copyleft (weak) (obligation/WARNING)': 49, 'Jurisdiction specific (other/WARNING)': 39, 'Display additional information (obligation/WARNING)': 35, 'Written offer to request source code (obligation/WARNING)': 32, 'Copyleft (strong) (obligation/WARNING)': 29, 'Display acknowledgement message (obligation/WARNING)': 27, 'Standard patent retaliation (other/WARNING)': 23, 'Deprecated License (other/INFORMATION)': 21, 'Doing Business with US (other/ALARM)': 19, 'Anti-Tivoization (obligation/WARNING)': 13, 'Severe patent retaliation (other/ALARM)': 11, 'Usage notice in advertisement (obligation/INFORMATION)': 10, 'Copyleft (network protective) (obligation/ALARM)': 8, 'Anti-DRM (obligation/WARNING)': 6, 'Unclear or Ambiguous (other/ALARM)': 5, 'Limitation (limitation/WARNING)': 5, 'Public Domain (other/INFORMATION)': 1})\n",
      "Label distribution after oversampling: Counter({'Use in distributed software (right/INFORMATION)': 10469, 'Display copyright notice (obligation/INFORMATION)': 10436, 'Display license in the source (obligation/INFORMATION)': 10436, 'Display license in binary (obligation/INFORMATION)': 9820, 'Permissive (right/INFORMATION)': 6168, 'Declare modification in source code (obligation/WARNING)': 5300, 'Keep copy of source code available (obligation/WARNING)': 4699, 'Provide source code location (obligation/WARNING)': 4494, 'Endorsement prohibited (prohibition/INFORMATION)': 4431, 'Patent grant (other/INFORMATION)': 4277, 'No further restrictions permitted (prohibition/INFORMATION)': 3586, 'Copyleft (weak) (obligation/WARNING)': 2859, 'License upgrade allowed (right/INFORMATION)': 2846, 'Jurisdiction specific (other/WARNING)': 2462, 'Display additional information (obligation/WARNING)': 2291, 'Written offer to request source code (obligation/WARNING)': 2267, 'Copyleft (strong) (obligation/WARNING)': 1911, 'Standard patent retaliation (other/WARNING)': 1538, 'Display acknowledgement message (obligation/WARNING)': 1290, 'Deprecated License (other/INFORMATION)': 1191, 'Doing Business with US (other/ALARM)': 1150, 'Anti-Tivoization (obligation/WARNING)': 1123, 'Severe patent retaliation (other/ALARM)': 801, 'Copyleft (network protective) (obligation/ALARM)': 764, 'Unclear or Ambiguous (other/ALARM)': 578, 'Usage notice in advertisement (obligation/INFORMATION)': 558, 'Limitation (limitation/WARNING)': 546, 'Public Domain (other/INFORMATION)': 467, 'Anti-DRM (obligation/WARNING)': 451})\n"
     ]
    }
   ],
   "source": [
    "# --- Oversampling using Binary Relevance with RandomOverSampler ---\n",
    "# Calculate label distribution before oversampling\n",
    "label_counts_before = Counter(label for labels in df[\"labels\"] for label in labels)\n",
    "\n",
    "# Initialize lists to store resampled data\n",
    "resampled_texts = []\n",
    "resampled_multihot_labels = []\n",
    "\n",
    "# Iterate over each label and apply oversampling to each binary problem\n",
    "for i in range(num_labels):\n",
    "    # Create a binary label array for the current label\n",
    "    binary_labels = multi_hot_labels[:, i]\n",
    "\n",
    "    # Reshape the features (texts) and labels for oversampling\n",
    "    X = np.array(df[\"text\"]).reshape(-1, 1)\n",
    "    y = binary_labels\n",
    "\n",
    "    # Apply RandomOverSampler\n",
    "    oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "    # Create new multi-hot encoded labels for the resampled data\n",
    "    for text, label in zip(X_resampled, y_resampled):\n",
    "        resampled_texts.append(text[0])  # Extract the text string from the array\n",
    "        new_labels = multi_hot_labels[df[df[\"text\"] == text[0]].index[0]].copy()\n",
    "        new_labels[i] = label  # Set the current label based on binary oversampling\n",
    "        resampled_multihot_labels.append(new_labels)\n",
    "\n",
    "# Create a new DataFrame from the resampled data\n",
    "df_resampled = pd.DataFrame({'text': resampled_texts, 'multi_hot_labels': resampled_multihot_labels})\n",
    "\n",
    "# Calculate label distribution after oversampling\n",
    "label_counts_after = Counter()\n",
    "for labels in df_resampled[\"multi_hot_labels\"]:\n",
    "    for i, label_present in enumerate(labels):\n",
    "        if label_present:\n",
    "            label_counts_after[mlb.classes_[i]] += 1\n",
    "\n",
    "print(\"Label distribution before oversampling:\", label_counts_before)\n",
    "print(\"Label distribution after oversampling:\", label_counts_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_resampled\n",
    "labels = torch.tensor(np.array(df[\"multi_hot_labels\"].tolist()), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split Data ---\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Tokenization ---\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Class ---\n",
    "class LicenseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = LicenseDataset(train_encodings, train_df[\"multi_hot_labels\"].tolist())\n",
    "test_dataset = LicenseDataset(test_encodings, test_df[\"multi_hot_labels\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "config.num_labels = num_labels\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = Counter()\n",
    "for labels in df[\"multi_hot_labels\"]:\n",
    "    for i, label_present in enumerate(labels):\n",
    "        if label_present:\n",
    "            label_counts[mlb.classes_[i]] += 1\n",
    "\n",
    "num_samples = len(df)\n",
    "weights = [num_samples / (num_labels * label_counts.get(label, 1)) for label in mlb.classes_]\n",
    "weights = torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights.to(device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# --- Move Model to Device ---\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,  # Changed to 10 epochs\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# --- Metrics Function ---\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = (pred.predictions > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics for multi-label classification\n",
    "    roc_auc = []\n",
    "    average_precision = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        if len(np.unique(labels[:, i])) > 1:\n",
    "            roc_auc.append(roc_auc_score(labels[:, i], preds[:, i]))\n",
    "        average_precision.append(average_precision_score(labels[:, i], preds[:, i]))\n",
    "\n",
    "    # Average the results\n",
    "    roc_auc = np.mean(roc_auc) if roc_auc else 0\n",
    "    average_precision = np.mean(average_precision)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1-score\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "\n",
    "    return {\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"average_precision\": average_precision,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd013ebb1544c88b618fa97d7b78ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6064, 'grad_norm': 0.8542879819869995, 'learning_rate': 4.9954296160877516e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4591, 'grad_norm': 0.5277919769287109, 'learning_rate': 4.990859232175503e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4178, 'grad_norm': 0.5543948411941528, 'learning_rate': 4.986288848263254e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# --- Train the Model ---\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2169\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\transformers\\trainer.py:2529\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- Train the Model ---\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c21a9ff8f745939b91ee50b8ec30c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.20971296727657318, 'eval_roc_auc': 0.6505465028120867, 'eval_average_precision': 0.43686081162034834, 'eval_accuracy': 0.1111111111111111, 'eval_precision': 0.9595959595959596, 'eval_recall': 0.7640750670241286, 'eval_f1': 0.8507462686567164, 'eval_runtime': 15.6833, 'eval_samples_per_second': 2.869, 'eval_steps_per_second': 0.064, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate the Model ---\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f10eb5a3964632a891389e01f6d6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6931, 'grad_norm': 0.7531012296676636, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.45}\n",
      "{'loss': 0.6903, 'grad_norm': 0.7990725040435791, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1881db4e3daa4b04a352f8f3ca0f4d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6866267919540405, 'eval_roc_auc': 0.5, 'eval_average_precision': 0.285823754789272, 'eval_accuracy': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 11.2802, 'eval_samples_per_second': 3.989, 'eval_steps_per_second': 0.089, 'epoch': 1.0}\n",
      "{'loss': 0.6832, 'grad_norm': 0.8112362623214722, 'learning_rate': 3e-06, 'epoch': 1.36}\n",
      "{'loss': 0.6733, 'grad_norm': 0.8657842874526978, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd839f8cb9345f98a2de270235a3aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6584188938140869, 'eval_roc_auc': 0.5, 'eval_average_precision': 0.285823754789272, 'eval_accuracy': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 11.1676, 'eval_samples_per_second': 4.03, 'eval_steps_per_second': 0.09, 'epoch': 2.0}\n",
      "{'loss': 0.6519, 'grad_norm': 1.2119724750518799, 'learning_rate': 5e-06, 'epoch': 2.27}\n",
      "{'loss': 0.5861, 'grad_norm': 1.1018736362457275, 'learning_rate': 6e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4f2439b2f64c0ca8df5137d754de2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4907730519771576, 'eval_roc_auc': 0.5, 'eval_average_precision': 0.285823754789272, 'eval_accuracy': 0.0, 'eval_precision': 0.9888888888888889, 'eval_recall': 0.4772117962466488, 'eval_f1': 0.64376130198915, 'eval_runtime': 11.7447, 'eval_samples_per_second': 3.832, 'eval_steps_per_second': 0.085, 'epoch': 3.0}\n",
      "{'loss': 0.5229, 'grad_norm': 1.1941916942596436, 'learning_rate': 7.000000000000001e-06, 'epoch': 3.18}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# --- Train the Model ---\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# --- Evaluate the Model ---\u001b[39;00m\n\u001b[0;32m     14\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2169\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\transformers\\trainer.py:2529\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Save the Model ---\n",
    "model.save_pretrained(\"./results/Roberta\")\n",
    "tokenizer.save_pretrained(\"./results/Roberta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
