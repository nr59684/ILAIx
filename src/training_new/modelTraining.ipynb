{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 0. Import modules\n",
    "# -----------------------------\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load Data\n",
    "# -----------------------------\n",
    "def load_license_data(json_folder):\n",
    "    license_data = []\n",
    "    for filename in os.listdir(json_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            license_name = filename[:-5]\n",
    "            filepath = os.path.join(json_folder, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                license_data.append({\n",
    "                    \"license_name\": license_name,\n",
    "                    \"family\": data[\"family\"],\n",
    "                    \"labels\": data[\"labels\"],\n",
    "                    \"text\": data[\"text\"],\n",
    "                })\n",
    "    return license_data\n",
    "\n",
    "json_folder = \"../../data/processed/preprocessed_licenses_json_2\"\n",
    "license_data = load_license_data(json_folder)\n",
    "df = pd.DataFrame(license_data)\n",
    "\n",
    "# Drop rows with missing or empty labels\n",
    "df.dropna(subset=[\"labels\"], inplace=True)\n",
    "df = df[df[\"labels\"].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Encode Labels\n",
    "# -----------------------------\n",
    "mlb = MultiLabelBinarizer()\n",
    "df[\"labels\"] = list(mlb.fit_transform(df[\"labels\"]))\n",
    "num_labels = len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Split Data (Train, Validation, Test)\n",
    "# -----------------------------\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8c2f2ceee7476897f9b3395e47e030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NPARSHO\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd6a40868b84e51bfd0dbdf0294178b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e3963ecbd24743a15d324926643a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87893ea00794435bab1b0c14a242d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaf4ffe6fcb42428378ca679baff8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debf5eac0b884ce2adce9d582baa19fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3043e0cc1954c69b75d1bcb95426b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38268f96dcd04e0390408892866a1de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NPARSHO\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-fr-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19a6f089b0c459381c132c0b1ccc1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd627102e55245f69c686a21bb280d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7bc1c92d0948fb8e7382c6bea50343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb7ef6d21a94f56b42ce63097a4351e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728d7987f33c4c78adc4da42bf7428e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d39d8780a411ea1a249765777a388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5266df493154b8784662272a23703ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f61f23b18a742789a3d6f442a1c25c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NPARSHO\\AppData\\Local\\anaconda3\\envs\\cuda2\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4083: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training size: 679\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. Data Augmentation\n",
    "# -----------------------------\n",
    "# Initialize backtranslation models globally for efficiency\n",
    "en_to_fr_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "fr_to_en_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "tokenizer_en_to_fr = MarianTokenizer.from_pretrained(en_to_fr_model_name)\n",
    "model_en_to_fr = MarianMTModel.from_pretrained(en_to_fr_model_name)\n",
    "tokenizer_fr_to_en = MarianTokenizer.from_pretrained(fr_to_en_model_name)\n",
    "model_fr_to_en = MarianMTModel.from_pretrained(fr_to_en_model_name)\n",
    "\n",
    "def backtranslate(text):\n",
    "    # Translate English to French\n",
    "    inputs = tokenizer_en_to_fr.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n",
    "    translated = model_en_to_fr.generate(**inputs)\n",
    "    french_text = tokenizer_en_to_fr.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "    # Translate French back to English\n",
    "    inputs = tokenizer_fr_to_en.prepare_seq2seq_batch([french_text], return_tensors=\"pt\")\n",
    "    translated = model_fr_to_en.generate(**inputs)\n",
    "    backtranslated_text = tokenizer_fr_to_en.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "    return backtranslated_text\n",
    "\n",
    "def augment_text(text, aug_p=0.2, aug_max=3):\n",
    "    \"\"\"\n",
    "    Randomly choose between contextual augmentation and backtranslation.\n",
    "    \"\"\"\n",
    "    augmenters = ['contextual', 'backtranslation']\n",
    "    chosen = random.choice(augmenters)\n",
    "    if chosen == 'contextual':\n",
    "        aug = naw.ContextualWordEmbsAug(\n",
    "            model_path='bert-base-uncased',\n",
    "            action=Action.SUBSTITUTE,\n",
    "            aug_p=aug_p,\n",
    "            aug_max=aug_max,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        try:\n",
    "            augmented = aug.augment(text)\n",
    "            return augmented[0] if isinstance(augmented, list) and len(augmented) > 0 else text\n",
    "        except Exception:\n",
    "            return text\n",
    "    elif chosen == 'backtranslation':\n",
    "        try:\n",
    "            return backtranslate(text)\n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "def augment_minority_classes(df, min_samples=50, aug_factor=3):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    # Compute label counts and identify minority labels\n",
    "    label_matrix = np.array(df[\"labels\"].tolist())\n",
    "    label_counts = label_matrix.sum(axis=0)\n",
    "    minority_labels = np.where(label_counts < min_samples)[0]\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        labels = row[\"labels\"]\n",
    "        sample_labels = np.where(np.array(labels) == 1)[0]\n",
    "        # Augment if any of the labels are in minority group\n",
    "        if any(label in minority_labels for label in sample_labels):\n",
    "            for _ in range(aug_factor):\n",
    "                new_text = augment_text(text)\n",
    "                augmented_texts.append(new_text)\n",
    "                augmented_labels.append(labels)\n",
    "    \n",
    "    augmented_df = pd.DataFrame({\n",
    "        \"text\": augmented_texts,\n",
    "        \"labels\": list(augmented_labels)\n",
    "    })\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    return pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "print(\"Original training size:\", len(train_df))\n",
    "train_df = augment_minority_classes(train_df, min_samples=50, aug_factor=3)\n",
    "print(\"Augmented training size:\", len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4725176b4edc49b8b5d33f8c5d3abe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408172d57dc84971bd82e010a1a637b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7620939fe2374540945fee330fc921e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Tokenization\n",
    "# -----------------------------\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    encoding = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "# Prepare Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize, batched=True)\n",
    "test_dataset = Dataset.from_pandas(test_df).map(tokenize, batched=True)\n",
    "\n",
    "# Set format for PyTorch (ensure \"labels\" is preserved)\n",
    "train_dataset = train_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset = val_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset = test_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Focal Loss Implementation\n",
    "# -----------------------------\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            logits, labels, reduction=\"none\"\n",
    "        )\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Custom Trainer with Focal Loss\n",
    "# -----------------------------\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\NPARSHO\\.cache\\huggingface\\hub\\models--answerdotai--ModernBERT-base\\snapshots\\8949b909ec900327062f0ebf497f51aef5e6f0c8\\config.json\n",
      "Model config ModernBertConfig {\n",
      "  \"_name_or_path\": \"answerdotai/ModernBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"ModernBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 50281,\n",
      "  \"classifier_activation\": \"gelu\",\n",
      "  \"classifier_bias\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"classifier_pooling\": \"mean\",\n",
      "  \"cls_token_id\": 50281,\n",
      "  \"decoder_bias\": true,\n",
      "  \"deterministic_flash_attn\": false,\n",
      "  \"embedding_dropout\": 0.0,\n",
      "  \"eos_token_id\": 50282,\n",
      "  \"global_attn_every_n_layers\": 3,\n",
      "  \"global_rope_theta\": 160000.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_activation\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Anti-DRM (obligation/WARNING)\",\n",
      "    \"1\": \"Anti-Tivoization (obligation/WARNING)\",\n",
      "    \"10\": \"Display license in binary (obligation/INFORMATION)\",\n",
      "    \"11\": \"Display license in the source (obligation/INFORMATION)\",\n",
      "    \"12\": \"Doing Business with US (other/ALARM)\",\n",
      "    \"13\": \"Endorsement prohibited (prohibition/INFORMATION)\",\n",
      "    \"14\": \"Jurisdiction specific (other/WARNING)\",\n",
      "    \"15\": \"Keep copy of source code available (obligation/WARNING)\",\n",
      "    \"16\": \"License upgrade allowed (right/INFORMATION)\",\n",
      "    \"17\": \"Limitation (limitation/WARNING)\",\n",
      "    \"18\": \"No further restrictions permitted (prohibition/INFORMATION)\",\n",
      "    \"19\": \"Patent grant (other/INFORMATION)\",\n",
      "    \"2\": \"Copyleft (network protective) (obligation/ALARM)\",\n",
      "    \"20\": \"Permissive (right/INFORMATION)\",\n",
      "    \"21\": \"Provide source code location (obligation/WARNING)\",\n",
      "    \"22\": \"Public Domain (other/INFORMATION)\",\n",
      "    \"23\": \"Severe patent retaliation (other/ALARM)\",\n",
      "    \"24\": \"Standard patent retaliation (other/WARNING)\",\n",
      "    \"25\": \"Unclear or Ambiguous (other/ALARM)\",\n",
      "    \"26\": \"Usage notice in advertisement (obligation/INFORMATION)\",\n",
      "    \"27\": \"Use in distributed software (right/INFORMATION)\",\n",
      "    \"28\": \"Written offer to request source code (obligation/WARNING)\",\n",
      "    \"3\": \"Copyleft (strong) (obligation/WARNING)\",\n",
      "    \"4\": \"Copyleft (weak) (obligation/WARNING)\",\n",
      "    \"5\": \"Declare modification in source code (obligation/WARNING)\",\n",
      "    \"6\": \"Deprecated License (other/INFORMATION)\",\n",
      "    \"7\": \"Display acknowledgement message (obligation/WARNING)\",\n",
      "    \"8\": \"Display additional information (obligation/WARNING)\",\n",
      "    \"9\": \"Display copyright notice (obligation/INFORMATION)\"\n",
      "  },\n",
      "  \"initializer_cutoff_factor\": 2.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1152,\n",
      "  \"label2id\": {\n",
      "    \"Anti-DRM (obligation/WARNING)\": \"0\",\n",
      "    \"Anti-Tivoization (obligation/WARNING)\": \"1\",\n",
      "    \"Copyleft (network protective) (obligation/ALARM)\": \"2\",\n",
      "    \"Copyleft (strong) (obligation/WARNING)\": \"3\",\n",
      "    \"Copyleft (weak) (obligation/WARNING)\": \"4\",\n",
      "    \"Declare modification in source code (obligation/WARNING)\": \"5\",\n",
      "    \"Deprecated License (other/INFORMATION)\": \"6\",\n",
      "    \"Display acknowledgement message (obligation/WARNING)\": \"7\",\n",
      "    \"Display additional information (obligation/WARNING)\": \"8\",\n",
      "    \"Display copyright notice (obligation/INFORMATION)\": \"9\",\n",
      "    \"Display license in binary (obligation/INFORMATION)\": \"10\",\n",
      "    \"Display license in the source (obligation/INFORMATION)\": \"11\",\n",
      "    \"Doing Business with US (other/ALARM)\": \"12\",\n",
      "    \"Endorsement prohibited (prohibition/INFORMATION)\": \"13\",\n",
      "    \"Jurisdiction specific (other/WARNING)\": \"14\",\n",
      "    \"Keep copy of source code available (obligation/WARNING)\": \"15\",\n",
      "    \"License upgrade allowed (right/INFORMATION)\": \"16\",\n",
      "    \"Limitation (limitation/WARNING)\": \"17\",\n",
      "    \"No further restrictions permitted (prohibition/INFORMATION)\": \"18\",\n",
      "    \"Patent grant (other/INFORMATION)\": \"19\",\n",
      "    \"Permissive (right/INFORMATION)\": \"20\",\n",
      "    \"Provide source code location (obligation/WARNING)\": \"21\",\n",
      "    \"Public Domain (other/INFORMATION)\": \"22\",\n",
      "    \"Severe patent retaliation (other/ALARM)\": \"23\",\n",
      "    \"Standard patent retaliation (other/WARNING)\": \"24\",\n",
      "    \"Unclear or Ambiguous (other/ALARM)\": \"25\",\n",
      "    \"Usage notice in advertisement (obligation/INFORMATION)\": \"26\",\n",
      "    \"Use in distributed software (right/INFORMATION)\": \"27\",\n",
      "    \"Written offer to request source code (obligation/WARNING)\": \"28\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"local_attention\": 128,\n",
      "  \"local_rope_theta\": 10000.0,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"mlp_dropout\": 0.0,\n",
      "  \"model_type\": \"modernbert\",\n",
      "  \"norm_bias\": false,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"pad_token_id\": 50283,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"reference_compile\": null,\n",
      "  \"sep_token_id\": 50282,\n",
      "  \"sparse_pred_ignore_index\": -100,\n",
      "  \"sparse_prediction\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.0.dev0\",\n",
      "  \"vocab_size\": 50368\n",
      "}\n",
      "\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 8. Model Setup\n",
    "# -----------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    label2id={label: str(i) for i, label in enumerate(mlb.classes_)},\n",
    "    id2label={str(i): label for i, label in enumerate(mlb.classes_)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9. Training Configuration\n",
    "# -----------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = torch.sigmoid(torch.tensor(p.predictions)).cpu().numpy() > 0.5\n",
    "    labels = p.label_ids\n",
    "    return {\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"hamming_loss\": hamming_loss(labels, preds)\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../model/Bert4.0\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    log_level=\"info\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: text, license_name, family. If text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 679\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 430\n",
      "  Number of trainable parameters = 149,627,165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1128cccf684e4851b61f6c32539e8417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0338, 'grad_norm': 0.3166827857494354, 'learning_rate': 1.9534883720930235e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0235, 'grad_norm': 0.17391929030418396, 'learning_rate': 1.9069767441860468e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0198, 'grad_norm': 0.08449245989322662, 'learning_rate': 1.86046511627907e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0184, 'grad_norm': 0.40008246898651123, 'learning_rate': 1.813953488372093e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c4c2692776445a9052522d36786208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-43\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-43\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.019957410171628, 'eval_f1_macro': 0.3462024504798812, 'eval_f1_micro': 0.7861885790172642, 'eval_hamming_loss': 0.10474951203643461, 'eval_runtime': 38.5264, 'eval_samples_per_second': 1.376, 'eval_steps_per_second': 0.104, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-180] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0161, 'grad_norm': 0.1603558212518692, 'learning_rate': 1.7674418604651163e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0163, 'grad_norm': 0.11087103188037872, 'learning_rate': 1.7209302325581396e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0142, 'grad_norm': 0.11978786438703537, 'learning_rate': 1.674418604651163e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0153, 'grad_norm': 0.1317322552204132, 'learning_rate': 1.6279069767441862e-05, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f43baefd94438686070e6f76838e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-86\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-86\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.018072564154863358, 'eval_f1_macro': 0.4128804492248801, 'eval_f1_micro': 0.8036410923276983, 'eval_hamming_loss': 0.09824333116460637, 'eval_runtime': 38.0437, 'eval_samples_per_second': 1.393, 'eval_steps_per_second': 0.105, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-43] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0134, 'grad_norm': 0.13580729067325592, 'learning_rate': 1.5813953488372095e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0111, 'grad_norm': 0.15702365338802338, 'learning_rate': 1.5348837209302328e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0118, 'grad_norm': 0.13769187033176422, 'learning_rate': 1.488372093023256e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0109, 'grad_norm': 0.2078876495361328, 'learning_rate': 1.441860465116279e-05, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40ab67c91b941239c7fbe7400e96534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-129\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-129\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.018003562465310097, 'eval_f1_macro': 0.459576141809573, 'eval_f1_micro': 0.8258575197889182, 'eval_hamming_loss': 0.08588158750813273, 'eval_runtime': 37.9465, 'eval_samples_per_second': 1.397, 'eval_steps_per_second': 0.105, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-86] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0094, 'grad_norm': 0.21581168472766876, 'learning_rate': 1.3953488372093025e-05, 'epoch': 3.02}\n",
      "{'loss': 0.0073, 'grad_norm': 0.24782583117485046, 'learning_rate': 1.3488372093023257e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0065, 'grad_norm': 0.39637893438339233, 'learning_rate': 1.302325581395349e-05, 'epoch': 3.49}\n",
      "{'loss': 0.0072, 'grad_norm': 0.1957440823316574, 'learning_rate': 1.2558139534883723e-05, 'epoch': 3.72}\n",
      "{'loss': 0.0073, 'grad_norm': 0.1842920482158661, 'learning_rate': 1.2093023255813954e-05, 'epoch': 3.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5afa50bb04240679c9b66fef5fb33df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-172\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-172\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02190111204981804, 'eval_f1_macro': 0.5560896391172055, 'eval_f1_micro': 0.8343711083437111, 'eval_hamming_loss': 0.08653220559531555, 'eval_runtime': 37.9979, 'eval_samples_per_second': 1.395, 'eval_steps_per_second': 0.105, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-129] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0049, 'grad_norm': 0.12041410803794861, 'learning_rate': 1.1627906976744187e-05, 'epoch': 4.19}\n",
      "{'loss': 0.005, 'grad_norm': 0.2955976724624634, 'learning_rate': 1.116279069767442e-05, 'epoch': 4.42}\n",
      "{'loss': 0.0046, 'grad_norm': 0.1834060549736023, 'learning_rate': 1.0697674418604651e-05, 'epoch': 4.65}\n",
      "{'loss': 0.0037, 'grad_norm': 0.14258672297000885, 'learning_rate': 1.0232558139534884e-05, 'epoch': 4.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aedf9bcf6e4d96b56d770403461332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-215\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-215\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.025575857609510422, 'eval_f1_macro': 0.5298706859995225, 'eval_f1_micro': 0.8297055057618438, 'eval_hamming_loss': 0.08653220559531555, 'eval_runtime': 38.0917, 'eval_samples_per_second': 1.391, 'eval_steps_per_second': 0.105, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-215] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.003, 'grad_norm': 0.11537239700555801, 'learning_rate': 9.767441860465117e-06, 'epoch': 5.12}\n",
      "{'loss': 0.0028, 'grad_norm': 0.2227599322795868, 'learning_rate': 9.30232558139535e-06, 'epoch': 5.35}\n",
      "{'loss': 0.0029, 'grad_norm': 0.07481295615434647, 'learning_rate': 8.837209302325582e-06, 'epoch': 5.58}\n",
      "{'loss': 0.0024, 'grad_norm': 0.07811135798692703, 'learning_rate': 8.372093023255815e-06, 'epoch': 5.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6b8c08e0e24f45a7a9332b530cd6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-258\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-258\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.027957003563642502, 'eval_f1_macro': 0.5689999521851361, 'eval_f1_micro': 0.8430379746835444, 'eval_hamming_loss': 0.08067664281067013, 'eval_runtime': 37.9694, 'eval_samples_per_second': 1.396, 'eval_steps_per_second': 0.105, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-172] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0025, 'grad_norm': 0.08982890099287033, 'learning_rate': 7.906976744186048e-06, 'epoch': 6.05}\n",
      "{'loss': 0.0017, 'grad_norm': 0.13825491070747375, 'learning_rate': 7.44186046511628e-06, 'epoch': 6.28}\n",
      "{'loss': 0.0024, 'grad_norm': 0.11251609772443771, 'learning_rate': 6.976744186046513e-06, 'epoch': 6.51}\n",
      "{'loss': 0.0018, 'grad_norm': 0.10781411081552505, 'learning_rate': 6.511627906976745e-06, 'epoch': 6.74}\n",
      "{'loss': 0.0021, 'grad_norm': 0.1306561827659607, 'learning_rate': 6.046511627906977e-06, 'epoch': 6.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9386588746eb485b83a6923e8f4c59af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-301\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-301\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02834116667509079, 'eval_f1_macro': 0.5546838381525192, 'eval_f1_micro': 0.8378033205619413, 'eval_hamming_loss': 0.0826284970722186, 'eval_runtime': 37.9501, 'eval_samples_per_second': 1.397, 'eval_steps_per_second': 0.105, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-301] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'grad_norm': 0.10110499709844589, 'learning_rate': 5.58139534883721e-06, 'epoch': 7.21}\n",
      "{'loss': 0.0015, 'grad_norm': 0.06459742039442062, 'learning_rate': 5.116279069767442e-06, 'epoch': 7.44}\n",
      "{'loss': 0.0014, 'grad_norm': 0.07300381362438202, 'learning_rate': 4.651162790697675e-06, 'epoch': 7.67}\n",
      "{'loss': 0.0012, 'grad_norm': 0.04339738190174103, 'learning_rate': 4.186046511627907e-06, 'epoch': 7.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142e8fd659cb46b59c3b00c7e4c0ae3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-344\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-344\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029063384979963303, 'eval_f1_macro': 0.5727424216553582, 'eval_f1_micro': 0.8418367346938775, 'eval_hamming_loss': 0.08067664281067013, 'eval_runtime': 37.9462, 'eval_samples_per_second': 1.397, 'eval_steps_per_second': 0.105, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-258] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'grad_norm': 0.07509707659482956, 'learning_rate': 3.72093023255814e-06, 'epoch': 8.14}\n",
      "{'loss': 0.0013, 'grad_norm': 0.08574200421571732, 'learning_rate': 3.2558139534883724e-06, 'epoch': 8.37}\n",
      "{'loss': 0.0011, 'grad_norm': 0.04984118789434433, 'learning_rate': 2.790697674418605e-06, 'epoch': 8.6}\n",
      "{'loss': 0.0011, 'grad_norm': 0.06070873886346817, 'learning_rate': 2.3255813953488376e-06, 'epoch': 8.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786afc3d93ce4790993fd1167ab657c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-387\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-387\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029338618740439415, 'eval_f1_macro': 0.5742807905023848, 'eval_f1_micro': 0.8421052631578947, 'eval_hamming_loss': 0.08002602472348731, 'eval_runtime': 38.0896, 'eval_samples_per_second': 1.391, 'eval_steps_per_second': 0.105, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [..\\..\\model\\Bert4.0\\checkpoint-344] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'grad_norm': 0.07109622657299042, 'learning_rate': 1.86046511627907e-06, 'epoch': 9.07}\n",
      "{'loss': 0.0009, 'grad_norm': 0.026853298768401146, 'learning_rate': 1.3953488372093025e-06, 'epoch': 9.3}\n",
      "{'loss': 0.0011, 'grad_norm': 0.058654945343732834, 'learning_rate': 9.30232558139535e-07, 'epoch': 9.53}\n",
      "{'loss': 0.0009, 'grad_norm': 0.04897912219166756, 'learning_rate': 4.651162790697675e-07, 'epoch': 9.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-430\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-430\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'grad_norm': 0.09619215875864029, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `ModernBertForSequenceClassification.forward` and have been ignored: __index_level_0__, text, license_name, family. If __index_level_0__, text, license_name, family are not expected by `ModernBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 53\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc20fab920bd441f9c960437d8f8f265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Bert4.0\\checkpoint-430\n",
      "Configuration saved in ../../model/Bert4.0\\checkpoint-430\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029989423230290413, 'eval_f1_macro': 0.574884529448395, 'eval_f1_micro': 0.8401015228426396, 'eval_hamming_loss': 0.08197787898503578, 'eval_runtime': 36.4537, 'eval_samples_per_second': 1.454, 'eval_steps_per_second': 0.11, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../../model/Bert4.0\\checkpoint-430 (score: 0.574884529448395).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 31296.18, 'train_samples_per_second': 0.217, 'train_steps_per_second': 0.014, 'train_loss': 0.006912035828586235, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=430, training_loss=0.006912035828586235, metrics={'train_runtime': 31296.18, 'train_samples_per_second': 0.217, 'train_steps_per_second': 0.014, 'total_flos': 2314178867374080.0, 'train_loss': 0.006912035828586235, 'epoch': 10.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 10. Train the Model\n",
    "# -----------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../model/Electra\n",
      "Configuration saved in ../../model/Electra\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../../model/Electra\\\\tokenizer_config.json',\n",
       " '../../model/Electra\\\\special_tokens_map.json',\n",
       " '../../model/Electra\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../../model/Electra\")  # Saves model and tokenizer\n",
    "tokenizer.save_pretrained(\"../../model/Electra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# Inference & Explainability Section\n",
    "# ===================================================\n",
    "# After training, set your model to evaluation mode.\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# A. Simple Prediction Function\n",
    "# -----------------------------\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.sigmoid(logits)\n",
    "    return probs.detach().cpu().numpy()\n",
    "\n",
    "# Example prediction\n",
    "sample_text = \"Your sample open source license text here.\"\n",
    "pred_probs = predict(sample_text)\n",
    "print(\"Predicted probabilities:\", pred_probs)\n",
    "\n",
    "# -----------------------------\n",
    "# B. Integrated Gradients with Captum\n",
    "# -----------------------------\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "def interpret_with_integrated_gradients(text, target_label_idx=0):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    def forward_func(input_ids, attention_mask):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        # Return logits for the target label\n",
    "        return logits[:, target_label_idx]\n",
    "    \n",
    "    ig = IntegratedGradients(forward_func)\n",
    "    attributions, delta = ig.attribute(inputs[\"input_ids\"],\n",
    "                                       additional_forward_args=(inputs[\"attention_mask\"],),\n",
    "                                       return_convergence_delta=True)\n",
    "    # Sum attributions across embedding dimensions\n",
    "    attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    return tokens, attributions_sum.cpu().detach().numpy(), delta.cpu().detach().numpy()\n",
    "\n",
    "tokens, attributions, delta = interpret_with_integrated_gradients(sample_text, target_label_idx=0)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Integrated Gradients Attributions:\", attributions)\n",
    "\n",
    "# -----------------------------\n",
    "# C. LIME Explanation\n",
    "# -----------------------------\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "def explain_with_lime(text):\n",
    "    explainer = LimeTextExplainer(class_names=mlb.classes_)\n",
    "    \n",
    "    def predict_proba(texts):\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs.detach().cpu().numpy()\n",
    "    \n",
    "    exp = explainer.explain_instance(text, predict_proba, num_features=10, num_samples=100)\n",
    "    return exp.as_list()\n",
    "\n",
    "lime_explanation = explain_with_lime(sample_text)\n",
    "print(\"LIME Explanation:\", lime_explanation)\n",
    "\n",
    "# -----------------------------\n",
    "# D. SHAP Explanation\n",
    "# -----------------------------\n",
    "import shap\n",
    "\n",
    "def explain_with_shap(text):\n",
    "    # Define a prediction function for SHAP\n",
    "    def predict_fn(texts):\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs.detach().cpu().numpy()\n",
    "    \n",
    "    # Create a SHAP explainer (this may take time on first run)\n",
    "    explainer = shap.Explainer(predict_fn, tokenizer)\n",
    "    shap_values = explainer([text])\n",
    "    return shap_values\n",
    "\n",
    "shap_values = explain_with_shap(sample_text)\n",
    "# Visualize the SHAP values for text explanation (this will open a plot in supported environments)\n",
    "shap.plots.text(shap_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
